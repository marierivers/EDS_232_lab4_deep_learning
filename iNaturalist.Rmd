---
title: "iNaturalist Lab"
author: "Marie Rivers"
date: "2/22/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
librarian::shelf(
  digest, dplyr, DT, glue, purrr, readr, stringr, tidyr, keras, tensorflow, here)
```

Apply deep learning and convnet techniques to a small subset of iNaturalist species imagery. These data were downloaded from the links provided at [github.com/visipedia/inat_comp:2021/](https://github.com/visipedia/inat_comp/tree/master/2021). This lab only draws from the Train Mini set of images.

The first step is to move the images into directories for the variety of models. The `keras::flow_images_from_firectory()` expects the first argument `directory` to contain one subdirectory per class. Since we are building models for two species `spp2` (binary) and ten species `spp10` (multiclass), plus we want to have `train` (n=30) and `test` (n=10) images assigned to each, we want an appropriate directory structure. 

```{r}
# path to folder containing species directories of images
dir_src <- "/courses/EDS232/inaturalist-2021/train_mini"
dir_dest <- here("inat/")
dir.create(dir_dest, showWarnings = F)

# get list of directories, one per species (n = 10,000 species)
dirs_spp <- list.dirs(dir_src, recursive = F, full.names = T)
n_spp <- length(dirs_spp)
```

```{r}
# set seed for reproducible results just before sampling otherwise you'll get different results, based on your username
Sys.info()[["user"]] %>% 
  digest::digest2int() %>% 
  set.seed()
i10 <- sample(1:n_spp, 10)

# show the 10 indices sampled of the 10,000 possible
i10
```

```{r}
# show the 10 species directory names
basename(dirs_spp)[i10]
```
```{r}
# show the first 2 species directory names
i2 <- i10[1:2]
basename(dirs_spp)[i2]
```
# Split the original images per species (n=50) into train (n=30), validate (n=10) and test (n=10)
```{r}
# setup data frame with source (src) and destination (dest) paths to images
d <- tibble(
  set = c(rep("spp2", 2), rep("spp10", 10)),
  dir_sp = c(dirs_spp[i2], dirs_spp[i10]),
  tbl_img = map(dir_sp, function(dir_sp){
    tibble(
      src_img = list.files(dir_sp, full.names = T),
      subset = c(rep("train", 30), rep("validation", 10), rep("test", 10))) })) %>% 
  unnest(tbl_img) %>% 
  mutate(
    sp = basename(dir_sp),
    img = basename(src_img),
    dest_img = glue("{dir_dest}/{set}/{subset}/{sp}/{img}"))
```

```{r}
# show source and destination for first 10 rows of tibble
d %>% 
  select(src_img, dest_img)
```

```{r}
# iterate over rows, creating directory if needed and copying files
d %>% 
  pwalk(function(src_img, dest_img, ...){
    dir.create(dirname(dest_img), recursive = T, showWarnings = F)
    file.copy(src_img, dest_img) })
```

```{r}
# uncomment to show the entire tree of your destination directory
# system(glue("tree {dir_dest}"))
```

```{r}
# 2 species
train_dir_spp2 <- here("inat/spp2/train/")
# train_00733_dir_spp2 <- here("train_dir_spp2/00733_Animalia_Arthropoda_Insecta_Hymenoptera_Cynipidae_Cynips_douglasii")
# train_04918_dir_spp2 <- here("train_dir_spp2/04918_Animalia_Chordata_Reptilia_Squamata_Colubridae_Lampropeltis_californiae")

validation_dir_spp2 <- here("inat/spp2/validation/")
# validation_00733_dir_spp2 <- ("validation_dir_spp2/00733_Animalia_Arthropoda_Insecta_Hymenoptera_Cynipidae_Cynips_douglasii")
# validation_04918_dir_spp2 <- here("validation_dir_spp2/04918_Animalia_Chordata_Reptilia_Squamata_Colubridae_Lampropeltis_californiae")

test_dir_spp2 <- here("inat/spp2/test/")
# test_00733_dir_spp2 <- here("test_dir_spp2/00733_Animalia_Arthropoda_Insecta_Hymenoptera_Cynipidae_Cynips_douglasii")
# test_04918_dir_spp2 <- here("test_dir_spp2/04918_Animalia_Chordata_Reptilia_Squamata_Colubridae_Lampropeltis_californiae")
```

```{r}
cat("total training 00733 images:", length(list.files(path = here(train_dir_spp2, "00733_Animalia_Arthropoda_Insecta_Hymenoptera_Cynipidae_Cynips_douglasii"))), "\n")
cat("total training 04918 images:", length(list.files(path = here(train_dir_spp2, "04918_Animalia_Chordata_Reptilia_Squamata_Colubridae_Lampropeltis_californiae"))), "\n")

cat("total validation 00733 images:", length(list.files(path = here(validation_dir_spp2, "00733_Animalia_Arthropoda_Insecta_Hymenoptera_Cynipidae_Cynips_douglasii"))), "\n")
cat("total validation 04918 images:", length(list.files(path = here(validation_dir_spp2, "04918_Animalia_Chordata_Reptilia_Squamata_Colubridae_Lampropeltis_californiae"))), "\n")

cat("total test 00733 images:", length(list.files(path = here(test_dir_spp2, "00733_Animalia_Arthropoda_Insecta_Hymenoptera_Cynipidae_Cynips_douglasii"))), "\n")
cat("total test 04918 images:", length(list.files(path = here(test_dir_spp2, "04918_Animalia_Chordata_Reptilia_Squamata_Colubridae_Lampropeltis_californiae"))), "\n")
```

```{r}
# 10 species
train_dir_spp10 <- here("inat/spp10/train/")
validation_dir_spp10 <- here("inat/spp10/validation/")
test_dir_spp10 <- here("inat/spp10/test/")
```

### Pre-process the images to be a consistent shape first (2 species)
see 5.2.4 Data preprocessing

Data should be formatted into appropriately pre-processed floating point tensors before being fed into a network. Currently, the data sites on a drive as JPEG files, so the steps for getting it into our network are:
- Read the picture files
- decode the JPEG content to RBG grids of pixels
- convert these into floating point tensors
- rescale the pixel values (between 0 adn 255) to the [0, 1] intervale (neural networks prefer to deal with small input values).

Keras includes the `image_generator()` function which can automatically turn image files on disk into batches of pre-processed tensors.
```{r}
# 2 species
# use image_data_generator to read images from directories
# all images will be rescaled by 1/255
train_datagen <- image_data_generator(rescale = 1/255)
validation_datagen <- image_data_generator(rescale = 1/255)
test_datagen <- image_data_generator(rescale = 1/255)

train_generator_spp2 <- flow_images_from_directory(
  # This is the target directory
  train_dir_spp2,
  # This is the data generator
  train_datagen,
  # All images will be resized to 150x150
  target_size = c(150, 150),
  batch_size = 5,
  # Since we use binary_crossentropy loss, we need binary labels
  class_mode = "binary"
)

validation_generator_spp2 <- flow_images_from_directory(
  validation_dir_spp2,
  validation_datagen,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "binary"
)

test_generator_spp2 <- flow_images_from_directory(
  test_dir_spp2,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "binary"
)
```
```{r}
batch <- generator_next(train_generator_spp2)
str(batch)
```
The generator yields batches of 150 x 150 RGB images (shape (5, 150, 150, 3)) and binary labels (shape(5)). There are 5 samples in each batch (the batch size).

### Processing images for 10 species 
```{r}
# 10 species
# use image_data_generator to read images from directories
# all images will be rescaled by 1/255
train_generator_spp10 <- flow_images_from_directory(
  # This is the target directory
  train_dir_spp10,
  # This is the data generator
  train_datagen,
  # All images will be resized to 150x150
  target_size = c(150, 150),
  batch_size = 5,
  # Since we use binary_crossentropy loss, we need binary labels
  class_mode = "categorical"
)

validation_generator_spp10 <- flow_images_from_directory(
  validation_dir_spp10,
  validation_datagen,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "categorical"
)

test_generator_spp10 <- flow_images_from_directory(
  test_dir_spp10,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 5,
  class_mode = "categorical"
)
```

Note: For binary-classification problems, you end the network with a single unit (a `Dense` layer of size 1) and a `sigmoid` activation. This unit will encode the probability that the network is looking at one class or the other.

Note: see table 4.1 for a cheatsheet on what loss function to use in various situations
```{r}
library(kableExtra)

activation_loss_function_table <- tibble::tribble(
  ~problem_type, ~last_layer_activation, ~loss_function,
  "binary classification", "sigmoid",  "binary_crossentropy",
  "multiclass, single-label classification", "softmax", "categorical_crossentropy",
  "multiclass, multilabel classification",  "sigmoid", "binary_crossentropy",
  "regression to arbitrary values", "none", "mse",
  "regression to values between 0 and 1", "sigmoid", "mae or binary_crossentropy")

activation_loss_function_table %>% 
  kable(col.names = c("Problem type", "Last-layer activation", "Loss function")) %>% 
  kable_paper(full_width = FALSE) %>% 
  row_spec(c(0), background = "lightgray")
```

# 2 Species (binary classification) - neural net
```{r}
# the model definition
model1 <- keras_model_sequential() %>%
  layer_flatten() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
# compile the model
model1 %>%  compile(
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)
```

```{r}
# fit model
history1 <- model1 %>% fit(
    train_generator_spp2,
    steps_per_epoch = 5,
    epochs = 15,
    validation_data = validation_generator_spp2,
    validation_steps = 1)
```

## accuracy metric and validation in the fitting process
xxx...don't confuse loss/accuracy for validation fitting vs test

```{r}
results1_train <- model1 %>% evaluate(train_generator_spp2)
results1_train
loss_1_train <- results1_train[1]
accuracy_1_train <- results1_train[2]
```

```{r}
results1_val <- model1 %>% evaluate(validation_generator_spp2)
results1_val
loss_1_val <- results1_val[1]
accuracy_1_val <- results1_val[2]
```

This 2 species neural network model has a loss of xxx and an accuracy of xxx.

## history plot
```{r}
plot(history1)
```

## Evaluate loss and accuracy on test model results. 
xxx
Note: Over-fitting is a particular concern when you have few training samples. Often, 2,000 images is considered "few samples" so the 30 samples used in this example will definitely be prone to over-fitting.

```{r}
results1_test <- model1 %>% evaluate(test_generator_spp2)
results1_test
loss_1_test <- results1_test[1]
accuracy_1_test <- results1_test[2]
```

## Compare standard neural network and convolutional neural network results

# 2 Species (binary classification) - convolutional neural net
draw from cat dog example (linked in assignment instructions)
```{r}
# convolutional neural net

# Because this is a binary-classification problem, we end the network with a single unit (a layer_dense() of size 1) and a sigmoid activation. This unit will encode the probability that the network is looking at one class or the other. 
model2 <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
summary(model2)
```

```{r}
# for the compilation step, go with the RMSprop optimizer as usual. Since we ended the network with a single sigmoid unit, we will use binary crossentropy as our loss.
model2 %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  metrics = c("accuracy"))
```

```{r}
# Fitting the model is generally the most time consuming computational process, so store a path to the model output and only run if not yet created.
history2 <- model2 %>% fit(
    train_generator_spp2,
    steps_per_epoch = 5,
    epochs = 15,
    validation_data = validation_generator_spp2,
    validation_steps = 1)
```

```{r}
history2
```

## accuracy metric and validation in the fitting process
```{r}
results2_train <- model2 %>% evaluate(train_generator_spp2)
results2_train
loss_2_train <- results2_train[1]
accuracy_2_train <- results2_train[2]
```

```{r}
results2_val <- model2 %>% evaluate(validation_generator_spp2)
results2_val
loss_2_val <- results2_val[1]
accuracy_2_val <- results2_val[2]
```

This 2 species convolutional neural network model has a loss of xxx and an accuracy of xxx.

## history plot
Plot the loss and accuracy of the model over the training and validation data during training:
```{r}
plot(history2)
```
These plots are characteristic of overfitting. Our training accuracy increases over time, until it reaches nearly 100%, while our validation accuracy stalls at 70-72%. Our validation loss reaches its minimum after around 12 epochs then stalls, while the training loss keeps decreasing approximately linearly until it reaches nearly 0.

Because we only have relatively few training samples (100), overfitting is going to be our number one concern. Techniques such as dropout, weight decay, and data augmentation can help mitigate overfitting.

xxx...see cat dog example in assignment link if you want to try data_augmentation

## Evaluate loss and accuracy on  test model results. 
```{r}
results2_test <- model2 %>% evaluate(test_generator_spp2)
results2_test
loss_2_test <- results2_test[1]
accuracy_2_test <- results2_test[2]
```

xxx

## Compare standard neural network and convolutional neural network results

# 10 Species (multi-class classification) - neural net
```{r}
# the model definition
model3 <- keras_model_sequential() %>%
  layer_flatten() %>% 
  layer_dense(units = 16, activation = "relu", input_shape = c(150, 150, 3)) %>%
  layer_dense(units = 16, activation = "relu") %>% 
  layer_dense(units = 1, activation = "softmax")
```

```{r}
# compile the model
model3 %>%  compile(
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```

```{r}
# fit model
history3 <- model3 %>% fit(
    train_generator_spp10,
    steps_per_epoch = 5,
    epochs = 14, # gives error when epochs = 15
    validation_data = validation_generator_spp10,
    validation_steps = 1)
```

```{r}
history3
```


## accuracy metric and validation in the fitting process
xxx...don't confuse loss/accuracy for validation fitting vs test

```{r}
results3_train <- model3 %>% evaluate(train_generator_spp10)
results3_train
loss_3_train <- results3_train[1]
accuracy_3_train <- results3_train[2]
```

```{r}
results3_val <- model3 %>% evaluate(validation_generator_spp2)
results3_val
loss_3_val <- results3_val[1]
accuracy_3_val <- results3_val[2]
```

This 10 species neural network model has a loss of xxx and an accuracy of xxx.

## history plot
```{r}
plot(history3)
```

## Evaluate loss and accuracy on  test model results. 
```{r}
results3_test <- model3 %>% evaluate(test_generator_spp10)
results3_test
loss_3_test <- results3_test[1]
accuracy_3_test <- results1_test[2]
```

## Compare standard neural network and convolutional neural network results
xxx

# 10 Species (multi-class classification) - convolutional neural net
```{r}
# convolutional neural net

# Because this is a binary-classification problem, we end the network with a single unit (a layer_dense() of size 1) and a sigmoid activation. This unit will encode the probability that the network is looking at one class or the other. 
model4 <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 3)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_flatten() %>% 
  layer_dense(units = 512, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax") # xxx...check this number; should it be 1 or 10?
```

```{r}
summary(model4)
```

```{r}
# for the compilation step, go with the RMSprop optimizer as usual. Since we ended the network with a single sigmoid unit, we will use categoical crossentropy as our loss.
model4 %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer_rmsprop(learning_rate = 1e-4),
  metrics = c("accuracy"))
```

```{r}
# Fitting the model is generally the most time consuming computational process, so store a path to the model output and only run if not yet created.
history4 <- model4 %>% fit( # note: fit_generator depricapted. use fit instead
    train_generator_spp10,
    steps_per_epoch = 5,
    epochs = 15, # xxx...adjust this to improve model fit
    validation_data = validation_generator_spp10,
    validation_steps = 1)
```

```{r}
history4
```

## accuracy metric and validation in the fitting process
```{r}
results4_train <- model4 %>% evaluate(train_generator_spp10)
results4_train
loss_4_train <- results4_train[1]
accuracy_4_train <- results4_train[2]
```

```{r}
results4_val <- model4 %>% evaluate(validation_generator_spp10)
results4_val
loss_4_val <- results4_val[1]
accuracy_4_val <- results4_val[2]
```

This 10 species convolutional neural network model has a loss of xxx and an accuracy of xxx.

## history plot.
Plot the loss and accuracy of the model over the training and validation data during training:
```{r}
plot(history4)
```
These plots are characteristic of overfitting. Our training accuracy increases over time, until it reaches nearly 100%, while our validation accuracy stalls at xxx. Our validation loss reaches its minimum after around 12 epochs then stalls, while the training loss keeps decreasing approximately linearly until it reaches nearly 0.

Because we only have relatively few training samples (100), overfitting is going to be our number one concern. Techniques such as dropout, weight decay, and data augmentation can help mitigate overfitting.

xxx...see cat dog example in assignment link if you want to try data_augmentation

## Evaluate loss and accuracy on  test model results. 
```{r}
results4_test <- model4 %>% evaluate(test_generator_spp10)
results4_test
loss_4_test <- results4_test[1]
accuracy_4_test <- results4_test[2]
```

## Compare standard neural network and convolutional neural network results
xxx

